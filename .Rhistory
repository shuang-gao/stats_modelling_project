source("~/Desktop/courses/MDA_postgraduate_UWO/Statistical_modelling/Project/Project_EDA.R", echo=TRUE)
Carv3 <- read_csv("/Users/andyc/Desktop/Masters/MDA 9159 Stat modelling/Project/Car_details_v3.csv")
#check dimensions of the data set
library(tidyverse)
dim(Carv3)
############################data cleaning
#checking for null value
table(is.na(Carv3))
#remove null values
carsv3 = na.omit(Carv3)
#check for dim of carsv3
dim(carsv3)
head(carsv3)
carsv3 = carsv3[,-1]
Carv3 <- read_csv("/Users/andyc/Desktop/Masters/MDA 9159 Stat modelling/Project/Car_details_v3.csv")
#check dimensions of the data set
library(tidyverse)
dim(Carv3)
############################data cleaning
#checking for null value
table(is.na(Carv3))
Carv3 <- read_csv("/Users/andyc/Desktop/Masters/MDA 9159 Stat modelling/Project/Car_details_v3.csv")
Carv3 <- read_csv("/Car_details_v3.csv")
Carv3 = read_csv("/Car_details_v3.csv")
Carv3 = read_csv("//Users/seangao/Desktop/courses/MDA_postgraduate_UWO/Statistical_modelling/Project/Car_details_v3.csv")
#check dimensions of the data set
library(tidyverse)
dim(Carv3)
############################data cleaning
#checking for null value
table(is.na(Carv3))
#remove null values
carsv3 = na.omit(Carv3)
#check for dim of carsv3
dim(carsv3)
head(carsv3)
carsv3 = carsv3[,-1]
dim(carsv3)
head(carsv3)
#take the first number set of torque, engine, mileage max power
for (i in 1:length(carsv3$torque)) {
carsv3$torque[i] = str_extract_all(carsv3$torque, "\\d+\\.*+\\d*+")[[i]][1]
carsv3$mileage[i] = str_extract_all(carsv3$mileage, "\\d+\\.*+\\d*+")[[i]]
carsv3$engine[i] = str_extract_all(carsv3$engine, "\\d+\\.*+\\d*+")[[i]]
carsv3$max_power[i] = str_extract_all(carsv3$max_power, "\\d+\\.*+\\d*+")[[i]]
}
dim(carsv3)
head(carsv3)
#Lets convert the selling price from rupees to CAD
carsv3$selling_price = carsv3$selling_price * 0.017
#data save point
cc = carsv3
#fail save
carsv3 = cc
#convert all numbers variable into numeric values
carsv3[,8:12] = sapply(carsv3[,8:12],as.numeric)
head(carsv3)
#convert some variables into factors
#carsv3$year = as.character(carsv3$year)
carsv3[sapply(carsv3, is.character)] <- lapply(carsv3[sapply(carsv3, is.character)],as.factor)
#carsv3[,4:7] = sapply(carsv3[,4:7],factor)
levels(as.factor(carsv3$owner))
str(carsv3)
sapply(carsv3[,c(4:7)], levels)
#The pairwise panel graph of numerical features/predictors
library(psych)
pairs.panels(carsv3[,-4:-7],
method = "pearson", # correlation method
hist.col = "#00AFBB",
density = TRUE,  # show density plots
ellipses = TRUE # show correlation ellipses
)
pairs(carsv3$selling_price ~., carsv3 , main="Scatterplot matrix")
# lets split the data in to training and testing
set.seed(10)
n = nrow( carsv3 )
idx = sample(n, n*0.6, replace = F)
car.tr = carsv3[idx,]
car.ts = carsv3[-idx,]
dim(carsv3);dim(car.tr);dim(car.ts)
# lets fit a linear regression
lm1 = lm(selling_price~. ,data = car.tr)
summary(lm1)
#lets check model diagnostics
plot(lm1)
#lets try removing the outliers  points
# The obs that are influential point
influ=which(cooks.distance(lm1) > 4/length(cooks.distance(lm1)))
influ
outliers= which((abs(rstandard(lm1)) > 2) & (cooks.distance(lm1) > 4 / length(cooks.distance(lm1))))
outliers
#remove the outliers
car.nout = carsv3[-outliers,]
n = nrow( car.nout )
idx = sample(n, n*0.6, replace = F)
car.trO = car.nout[idx,]
car.tsO = car.nout[-idx,]
dim(car.nout);dim(car.trO);dim(car.tsO)
#refiit model 1
# lets fit a linear regression
lm2 = lm(selling_price~. ,data = car.tr)
summary(lm2)
#lets check model diagnostics
plot(lm2)
#lets do some boxcox transfomation to see if the model assumption is corrected
library(MASS)
boxcox(lm1,lambda = seq(-0.05, 0.01, by = 0.001))
car.nout$selling_price
lambda = -0.005
lm3 <- lm(((car.tr$selling_price^(lambda)-1)/(lambda)) ~ ., data = car.tr)
summary(lm3)
plot(lm3)
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#test confirms that EV is violated and Normality is violated
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(y ~ selling_price + I(.^2), data = carsv3) # quadratic (true model)
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ . + I(.^2), data = carsv3) # quadratic (true model)
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ . , data = carsv3) # quadratic (true model)
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ poly(.,2) , data = carsv3) # quadratic (true model)
carsv3
summary(lm3)
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ .^2 , data = carsv3) # quadratic (true model)
summary(fit_2)
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~.+ .^2 , data = carsv3) # quadratic
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~.+ .^2 , data = carsv3) # quadratic
summary(fit_2)
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ .^2 , data = carsv3) # quadratic
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ .+.^2 , data = carsv3) # quadratic
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ .+I(.^2) , data = carsv3) # quadratic
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ fuelLPG + fuelLPG^2 , data = carsv3) # quadratic
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ max_power + max_power^2 , data = carsv3) # quadratic
summary(fit_2)
fit_2 = lm(y ~ x + I(x^2), data = sim_data) # quadratic (true model)
# This is the true reg function
f = function(x) {
x ^ 2
}
# This function will generate simulated data from f(x)
get_sim_data = function(f, sample_size = 100) {
x = runif(n = sample_size, min = 0, max = 1)
y = f(x) + rnorm(n = sample_size, mean = 0, sd = 0.3)
data.frame(x, y)
}
set.seed(10)
sim_data = get_sim_data(f, sample_size = 100)
# Fitting different models
fit_0 = lm(y ~ 1, data = sim_data) # intercept
fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(y ~ x + I(x^2), data = sim_data) # quadratic (true model)
fit_4 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = sim_data) # quartic
summary(fit_2)
head(carsv3)
pairs(carsv3$selling_price ~., carsv3 , main="Scatterplot matrix")
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ . + I(selling_price^2) +
I(km_driven^2 + I(mileage^2) +I(engine^2))  , data = carsv3) # quadratic
fit_4 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = sim_data) # quartic
#fit_0 = lm(y ~ 1, data = sim_data) # intercept
#fit_1 = lm(y ~ x, data = sim_data) # linear
fit_2 = lm(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)  , data = carsv3) # quadratic
fit_4 = lm(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)+ I(selling_price^3) +
I(km_driven^3) + I(mileage^3) +I(engine^3) + I(selling_price^4) +
I(km_driven^4) + I(mileage^4) +I(engine^4) , data = carsv3)  # quartic
summary(fit_2)
head(carsv3)
fit_0 = lm(selling_price ~ 1, data = carsv3) # intercept
fit_1 = lm(selling_price ~ ., data = carsv3) # linear
fit_2 = lm(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)  , data = carsv3) # quadratic
fit_4 = lm(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)+ I(selling_price^3) +
I(km_driven^3) + I(mileage^3) +I(engine^3) + I(selling_price^4) +
I(km_driven^4) + I(mileage^4) +I(engine^4) , data = carsv3)  # quartic
summary(fit_2)
head(carsv3)
set.seed(1)
plot(selling_price ~ ., data = carsv3)
c("intercept", "linear", "quadratic",  "quartic", "truth"),
col = c("red", "blue", "green", "orange", "black"), lty = c(2, 3, 4, 5, 1), lwd = 2)
legend("topleft",
c("intercept", "linear", "quadratic",  "quartic", "truth"),
col = c("red", "blue", "green", "orange", "black"), lty = c(2, 3, 4, 5, 1), lwd = 2)
lines(grid, f(grid), col = "black", lwd = 5)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)),
col = "red", lwd = 2, lty = 2)
plot(y ~ x, data = sim_data)
set.seed(1)
plot(y ~ x, data = sim_data)
grid = seq(from = 0, to = 1, by = 0.01)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)),
col = "red", lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)),
col = "blue", lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)),
col = "green", lwd = 2, lty = 4)
lines(grid, predict(fit_4, newdata = data.frame(x = grid)),
col = "orange", lwd = 2, lty = 5)
lines(grid, f(grid), col = "black", lwd = 5)
legend("topleft",
c("intercept", "linear", "quadratic",  "quartic", "truth"),
col = c("red", "blue", "green", "orange", "black"), lty = c(2, 3, 4, 5, 1), lwd = 2)
set.seed(10)
n_sims = 1000 # simulation runs
n_models = 4 # we compare 4 models
x0 = 0.98
predictions = matrix(0, nrow = n_sims, ncol = n_models)
sim_data = get_sim_data(f, sample_size = 100)
plot(y ~ x, data = sim_data, col = "white", xlim = c(0.75, 1), ylim = c(0, 1.5))
# This plot shows that
# 1) fit_0 (red, constant) has low variance and high bias
# 2) fit_4 (orange, quartic) model has low bias and high variance
for (i in 1:n_sims)
n_sims = 1000 # simulation runs
n_models = 4 # we compare 4 models
x0 = 0.98
predictions = matrix(0, nrow = n_sims, ncol = n_models)
sim_data = get_sim_data(f, sample_size = 100)
plot(y ~ x, data = sim_data, col = "white", xlim = c(0.75, 1), ylim = c(0, 1.5))
# This plot shows that
# 1) fit_0 (red, constant) has low variance and high bias
# 2) fit_4 (orange, quartic) model has low bias and high variance
for (i in 1:n_sims)
{
sim_data = get_sim_data(f, sample_size = 100)
fit_0 = lm(y ~ 1, data = sim_data)
fit_1 = lm(y ~ x, data = sim_data)
fit_2 = lm(y ~ x + I(x^2), data = sim_data)
fit_4 = lm(y ~ x + I(x^2) + I(x^3) + I(x^4), data = sim_data)
#lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = "red", lwd = 1)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = "blue", lwd = 1)
#lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = "green", lwd = 1)
lines(grid, predict(fit_4, newdata = data.frame(x = grid)), col = "orange", lwd = 1)
predictions[i, ] = c(
predict(fit_0, newdata = data.frame(x = x0)),
predict(fit_1, newdata = data.frame(x = x0)),
predict(fit_2, newdata = data.frame(x = x0)),
predict(fit_4, newdata = data.frame(x = x0))
)
}
Carv3 = read_csv("//Users/seangao/Desktop/courses/MDA_postgraduate_UWO/Statistical_modelling/Project/Car_details_v3.csv")
#check dimensions of the data set
library(tidyverse)
dim(Carv3)
############################data cleaning
#checking for null value
table(is.na(Carv3))
#remove null values
carsv3 = na.omit(Carv3)
#check for dim of carsv3
dim(carsv3)
head(carsv3)
carsv3 = carsv3[,-1]
dim(carsv3)
head(carsv3)
#take the first number set of torque, engine, mileage max power
for (i in 1:length(carsv3$torque)) {
carsv3$torque[i] = str_extract_all(carsv3$torque, "\\d+\\.*+\\d*+")[[i]][1]
carsv3$mileage[i] = str_extract_all(carsv3$mileage, "\\d+\\.*+\\d*+")[[i]]
carsv3$engine[i] = str_extract_all(carsv3$engine, "\\d+\\.*+\\d*+")[[i]]
carsv3$max_power[i] = str_extract_all(carsv3$max_power, "\\d+\\.*+\\d*+")[[i]]
}
dim(carsv3)
head(carsv3)
#Lets convert the selling price from rupees to CAD
carsv3$selling_price = carsv3$selling_price * 0.017
#data save point
cc = carsv3
#fail save
carsv3 = cc
#convert all numbers variable into numeric values
carsv3[,8:12] = sapply(carsv3[,8:12],as.numeric)
head(carsv3)
#convert some variables into factors
#carsv3$year = as.character(carsv3$year)
carsv3[sapply(carsv3, is.character)] <- lapply(carsv3[sapply(carsv3, is.character)],as.factor)
#carsv3[,4:7] = sapply(carsv3[,4:7],factor)
levels(as.factor(carsv3$owner))
str(carsv3)
sapply(carsv3[,c(4:7)], levels)
#The pairwise panel graph of numerical features/predictors
library(psych)
pairs.panels(carsv3[,-4:-7],
method = "pearson", # correlation method
hist.col = "#00AFBB",
density = TRUE,  # show density plots
ellipses = TRUE # show correlation ellipses
)
pairs(carsv3$selling_price ~., carsv3 , main="Scatterplot matrix")
# lets split the data in to training and testing
set.seed(10)
n = nrow( carsv3 )
idx = sample(n, n*0.6, replace = F)
car.tr = carsv3[idx,]
car.ts = carsv3[-idx,]
dim(carsv3);dim(car.tr);dim(car.ts)
# lets fit a linear regression
lm1 = lm(selling_price~. ,data = car.tr)
summary(lm1)
fit_0 = lm(selling_price ~ 1, data = car.tr) # intercept
fit_1 = lm(selling_price ~ ., data = car.tr) # linear
fit_2 = lm(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)  , data = car.tr) # quadratic
fit_4 = lm(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)+ I(selling_price^3) +
I(km_driven^3) + I(mileage^3) +I(engine^3) + I(selling_price^4) +
I(km_driven^4) + I(mileage^4) +I(engine^4) , data = car.tr)  # quartic
set.seed(10)
n_sims = 1000 # simulation runs
fit_0 = lm(selling_price ~ 1, data = car.tr,trControl = train.control) # intercept
fit_0 = train(selling_price ~ 1, data = car.tr,trControl = train.control) # intercept
# Model comparison (linear/quadratic/quartic)
library(tidyverse)
library(caret)
train.control <- trainControl(method = "cv", number = 10) # Prepares for cv scoring
print(fit_4)
print(fit_0)
print(fit_1)
print(fit_2)
print(fit_4)
fit_0 = train(selling_price ~ 1, data = car.tr,trControl = train.control) # intercept
fit_1 = train(selling_price ~ ., data = car.tr,method = "lm",trControl = train.control) # linear
fit_2 = train(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)  , data = car.tr,method = "lm",
trControl = train.control) # quadratic
fit_4 = train(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)+ I(selling_price^3) +
I(km_driven^3) + I(mileage^3) +I(engine^3) + I(selling_price^4) +
I(km_driven^4) + I(mileage^4) +I(engine^4) , data = car.tr,
method = "lm",trControl = train.control)  # quartic
print(fit_0)
print(fit_1)
print(fit_2)
print(fit_4)
# The quadratic model has the smallest RMSE and highest Rsquared.
# We apply the quadratic model on the test data:
ypred = predict(fit_2, car.ts)
ypred
ytest = car.ts$selling_price
mse = mean((ypred-ytest)^2)
mse
tss = sum(ytest-mean(ytest))^2)
tss = sum(ytest-mean(ytest)^2)
rss = sum((ypred-ytest)^2)
tss = sum((ytest-mean(ytest))^2)
rsq
rsq =1-rss/tss
rsq
source("~/Downloads/Project_EDA.R", echo=TRUE)
source("~/Desktop/courses/MDA_postgraduate_UWO/Statistical_modelling/Project/Project_EDA 2.46.56 PM.R", echo=TRUE)
Carv3 <- read_csv("/Users/andyc/Desktop/Masters/MDA 9159 Stat modelling/Project/Car_details_v3.csv")
source("~/Desktop/courses/MDA_postgraduate_UWO/Statistical_modelling/Project/Project_EDA.R", echo=TRUE)
source("~/Desktop/courses/MDA_postgraduate_UWO/Statistical_modelling/Project/Project_EDA.R", echo=TRUE)
# Model comparison (linear/quadratic/quartic)
library(tidyverse)
library(caret)
train.control <- trainControl(method = "cv", number = 10) # Prepares for cv scoring
fit_0 = train(selling_price ~ 1, data = car.tr,trControl = train.control) # intercept
fit_0 = lm(selling_price ~ 1, data = car.tr) # intercept
fit_1 = train(selling_price ~ ., data = car.tr,method = "lm",trControl = train.control) # linear
fit_2 = train(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)  , data = car.tr,method = "lm",
trControl = train.control) # quadratic
fit_4 = train(selling_price ~ . + I(selling_price^2) +
I(km_driven^2) + I(mileage^2) +I(engine^2)+ I(selling_price^3) +
I(km_driven^3) + I(mileage^3) +I(engine^3) + I(selling_price^4) +
I(km_driven^4) + I(mileage^4) +I(engine^4) , data = car.tr,
method = "lm",trControl = train.control)  # quartic
print(fit_0)
print(fit_1)
print(fit_2)
print(fit_4)
# The quadratic model has the smallest RMSE and highest Rsquared.
# We apply the quadratic model on the test data:
ypred = predict(fit_2, car.ts)
ytest = car.ts$selling_price
mse = mean((ypred-ytest)^2)
rss = sum((ypred-ytest)^2)
tss = sum((ytest-mean(ytest))^2)
rsq =1-rss/tss
rsq
0.95695310.9569531
